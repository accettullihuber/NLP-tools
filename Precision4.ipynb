{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80a6b798",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6043e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "#import the custom functions\n",
    "import MFunctions as mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89b5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load verb_symilarity from which we extract the pairs\n",
    "simdict = mf.verb_similarity(\"SimVerb-3500.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5db54c",
   "metadata": {},
   "source": [
    "# Define some functions for the precision computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6985577a",
   "metadata": {},
   "source": [
    "## Here the precision is computed by averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ecb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We decided to compute the precision by averaging over the precision in each category, which scales well\n",
    "# for cases where the sample sizes are very different for syn/non/ant (we have ~300 syn ~100 ant ~2000 none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f56cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synvsant1(file_name):\n",
    "    #Using observable vectors with cosine distance\n",
    "\n",
    "    #Compute the mean and the standard deviation for the SYNONYMS\n",
    "    syn1 = mf.averaged_product(simdict,'SYNONYMS',file_name,\"std_dev\")\n",
    "\n",
    "    #Compute the mean and the standard deviation for the ANTONYMS\n",
    "    ant1 = mf.averaged_product(simdict,'ANTONYMS',file_name,\"std_dev\")\n",
    "    \n",
    "    #Use the means and deviations to set the divide between synonyms and antonyms\n",
    "    divide1 = ant1[0] + ant1[1]/syn1[1] * (syn1[0]-ant1[0])/2\n",
    "    \n",
    "    # Use the earlier defined function to compute the list of scalar products of the verbs and compare with divide\n",
    "    prodlist1 = mf.averaged_product_list(simdict,'SYNONYMS',file_name,\"std_dev\")\n",
    "\n",
    "    prodlist5 = mf.averaged_product_list(simdict,'ANTONYMS',file_name,\"std_dev\")\n",
    "    \n",
    "    # Compare and count the number of synonyms we get right\n",
    "    syncomparison1 = [a > divide1 for a in list(prodlist1.values())]\n",
    "\n",
    "    # Compare and count the number of antonyms we get right\n",
    "    antcomparison1 = [a < divide1 for a in list(prodlist5.values())]\n",
    "\n",
    "    #Compute the average precision\n",
    "    precision1 = 1/2*(syncomparison1.count(True)/len(syncomparison1) + antcomparison1.count(True)/len(antcomparison1))\n",
    "    \n",
    "    return precision1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2077df48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synvsnonevsant1(file_name):\n",
    "    #Using observable vectors with cosine distance\n",
    "\n",
    "    #Compute the mean and the standard deviation for the SYNONYMS\n",
    "    syn1 = mf.averaged_product(simdict,'SYNONYMS',file_name,\"std_dev\")\n",
    "    \n",
    "    #Compute the mean and the standard deviation for the NONE\n",
    "    non1 = mf.averaged_product(simdict,'NONE',file_name,\"std_dev\")\n",
    "\n",
    "    #Compute the mean and the standard deviation for the ANTONYMS\n",
    "    ant1 = mf.averaged_product(simdict,'ANTONYMS',file_name,\"std_dev\")\n",
    "\n",
    "    #Use the means and deviations to set the divide between synonyms and none\n",
    "    divide1 = non1[0] + non1[1]/syn1[1] * (syn1[0]-non1[0])/2\n",
    "\n",
    "    #Use the means and deviations to set the divide between none and antonyms\n",
    "    divide5 = ant1[0] + ant1[1]/non1[1] * (non1[0]-ant1[0])/2\n",
    "    \n",
    "    # Use the earlier defined function to compute the list of scalar products of the verbs and compare with divide\n",
    "    prodlist1 = mf.averaged_product_list(simdict,'SYNONYMS',file_name,\"std_dev\")\n",
    "\n",
    "    prodlist5 = mf.averaged_product_list(simdict,'ANTONYMS',file_name,\"std_dev\")\n",
    "\n",
    "    prodlist9 = mf.averaged_product_list(simdict,'NONE',file_name,\"std_dev\")\n",
    "\n",
    "    # Compare and count the number of synonyms we get right\n",
    "    syncomparison1 = [a > divide1 for a in list(prodlist1.values())]\n",
    "\n",
    "    # Compare and count the number of none we get right\n",
    "    noncomparison1 = [a > divide5 and a < divide1 for a in list(prodlist9.values())]\n",
    "\n",
    "    # Compare and count the number of antonyms we get right\n",
    "    antcomparison1 = [a < divide5 for a in list(prodlist5.values())]\n",
    "\n",
    "    #Compute the average precision\n",
    "    precision1 = 1/3*(syncomparison1.count(True)/len(syncomparison1) + antcomparison1.count(True)/len(antcomparison1) + noncomparison1.count(True)/len(noncomparison1))\n",
    "\n",
    "    return precision1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c7a5b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synvsant2(file_name,myseed1,myseed2,samplesize1,samplesize2):\n",
    "    #Since here we use just a subset of the pairs of synonyms and antonyms to determine the means and deviation, we have to do this by hand\n",
    "\n",
    "    #Compute the products for all the observable deviation vectors associated to the SYNONYMS pairs\n",
    "    all1 = mf.averaged_product_list(simdict,'SYNONYMS',file_name,\"std_dev\")\n",
    "\n",
    "    #The subset from which we copute the means is randomly selected, we seed the random generator for repeatable results. Chosen seed 7\n",
    "    random.seed(myseed1)\n",
    "\n",
    "    mysample = random.sample(list(all1),samplesize1)\n",
    "\n",
    "    #Find complementary sample where we will predict and test\n",
    "    mycomplsample = set(all1.keys()) - set(mysample)\n",
    "\n",
    "    #Compute the means and standard deviations\n",
    "    syn1 = [np.mean(np.array(list(map(all1.get,mysample)))),np.std(np.array(list(map(all1.get,mysample))))]\n",
    "\n",
    "    #Now the same thing for the antonyms\n",
    "\n",
    "    #Compute the products for all the observable deviation vectors associated to the SYNONYMS pairs\n",
    "    all5 = mf.averaged_product_list(simdict,'ANTONYMS',file_name,\"std_dev\")\n",
    "\n",
    "    #The subset from which we copute the means is randomly selected, we seed the random generator for repeatable results. Chosen seed 13\n",
    "    #random.seed(117) #Interesting values come out for this seed\n",
    "    random.seed(myseed2)\n",
    "\n",
    "    mysample2 = random.sample(list(all5),samplesize2)\n",
    "\n",
    "    #Find complementary sample where we will predict and test\n",
    "    mycomplsample2 = set(all5.keys()) - set(mysample2)\n",
    "\n",
    "    #Compute the means and standard deviations\n",
    "    ant1 = [np.mean(np.array(list(map(all5.get,mysample2)))),np.std(np.array(list(map(all5.get,mysample2))))]\n",
    "\n",
    "    #Use the means and deviations to set the divide between synonyms and antonyms\n",
    "    divide1 = ant1[0] + ant1[1]/syn1[1] * (syn1[0]-ant1[0])/2\n",
    "    \n",
    "    # Next we test on the complementary sample\n",
    "    # Compare and count the number of synonyms we get right\n",
    "    syncomparison1 = [a > divide1 for a in list(map(all1.get,mycomplsample))]\n",
    "\n",
    "    # Compare and count the number of antonyms we get right\n",
    "    antcomparison1 = [a < divide1 for a in list(map(all5.get,mycomplsample2))]\n",
    "\n",
    "    #Compute the average precision\n",
    "    precision1 = 1/2*(syncomparison1.count(True)/len(syncomparison1) + antcomparison1.count(True)/len(antcomparison1))\n",
    "\n",
    "    return precision1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b727553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synvsnonevsant2(file_name,myseed1,myseed2,myseed3,samplesize1,samplesize2,samplesize3):\n",
    "    \n",
    "    #Since here we use just a subset of the pairs of synonyms and antonyms to determine the means and deviation, we have to do this by hand\n",
    "\n",
    "    #Compute the products for all the observable deviation vectors associated to the SYNONYMS pairs\n",
    "    all1 = mf.averaged_product_list(simdict,'SYNONYMS',file_name,\"std_dev\")\n",
    "\n",
    "    #The subset from which we copute the means is randomly selected, we seed the random generator for repeatable results. Chosen seed 7\n",
    "    random.seed(myseed1)\n",
    "\n",
    "    mysample = random.sample(list(all1),samplesize1)\n",
    "\n",
    "    #Find complementary sample where we will predict and test\n",
    "    mycomplsample = set(all1.keys()) - set(mysample)\n",
    "\n",
    "    #Compute the means and standard deviations\n",
    "    syn1 = [np.mean(np.array(list(map(all1.get,mysample)))),np.std(np.array(list(map(all1.get,mysample))))]\n",
    "\n",
    "    #Now the same thing for the antonyms\n",
    "\n",
    "    #Compute the products for all the observable deviation vectors associated to the SYNONYMS pairs\n",
    "    all5 = mf.averaged_product_list(simdict,'ANTONYMS',file_name,\"std_dev\")\n",
    "    \n",
    "    #The subset from which we copute the means is randomly selected, we seed the random generator for repeatable results. Chosen seed 13\n",
    "    random.seed(myseed2)\n",
    "\n",
    "    mysample2 = random.sample(list(all5),samplesize2)\n",
    "\n",
    "    #Find complementary sample where we will predict and test\n",
    "    mycomplsample2 = set(all5.keys()) - set(mysample2)\n",
    "\n",
    "    #Compute the means and standard deviations\n",
    "    ant1 = [np.mean(np.array(list(map(all5.get,mysample2)))),np.std(np.array(list(map(all5.get,mysample2))))]\n",
    "    \n",
    "    #Finally the same for None\n",
    "\n",
    "    #Compute the products for all the observable deviation vectors associated to the SYNONYMS pairs\n",
    "    all9 = mf.averaged_product_list(simdict,'NONE',file_name,\"std_dev\")\n",
    "    \n",
    "    #The subset from which we copute the means is randomly selected, we seed the random generator for repeatable results. Chosen seed 13\n",
    "    random.seed(myseed3)\n",
    "\n",
    "    mysample3 = random.sample(list(all9),samplesize3)\n",
    "\n",
    "    #Find complementary sample where we will predict and test\n",
    "    mycomplsample3 = set(all9.keys()) - set(mysample3)\n",
    "\n",
    "    #Compute the means and standard deviations\n",
    "    non1 = [np.mean(np.array(list(map(all9.get,mysample3)))),np.std(np.array(list(map(all9.get,mysample3))))]\n",
    "    \n",
    "    #Use the means and deviations to set the divide between synonyms and none\n",
    "    divide1 = non1[0] + non1[1]/syn1[1] * (syn1[0]-non1[0])/2\n",
    "\n",
    "    #Use the means and deviations to set the divide between none and antonyms\n",
    "    divide5 = ant1[0] + ant1[1]/non1[1] * (non1[0]-ant1[0])/2\n",
    "    \n",
    "    # Next we test on the complementary sample\n",
    "    # Compare and count the number of synonyms we get right\n",
    "    syncomparison1 = [a > divide1 for a in list(map(all1.get,mycomplsample))]\n",
    "\n",
    "\n",
    "    # Compare and count the number of none we get right\n",
    "    noncomparison1 = [a > divide5 and a < divide1 for a in list(map(all9.get,mycomplsample3))]\n",
    "    \n",
    "    # Compare and count the number of antonyms we get right\n",
    "    antcomparison1 = [a < divide5 for a in list(map(all5.get,mycomplsample2))]\n",
    "    \n",
    "    #Compute the average precision\n",
    "    precision1 = 1/3*(syncomparison1.count(True)/len(syncomparison1) + antcomparison1.count(True)/len(antcomparison1) + noncomparison1.count(True)/len(noncomparison1))\n",
    "    \n",
    "    return precision1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ab761f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypohyper1(file_name):\n",
    "    #Using observable vectors with cosine distance\n",
    "\n",
    "    #Compute the mean and the standard deviation for the SYNONYMS\n",
    "    syn1 = mf.averaged_product(simdict,'COHYPONYMS',file_name,\"std_dev\")\n",
    "\n",
    "    #Compute the mean and the standard deviation for the ANTONYMS\n",
    "    ant1 = mf.averaged_product(simdict,'HYPER/HYPONYMS',file_name,\"std_dev\")\n",
    "    \n",
    "    #Use the means and deviations to set the divide between synonyms and antonyms\n",
    "    divide1 = ant1[0] + ant1[1]/syn1[1] * (syn1[0]-ant1[0])/2\n",
    "    \n",
    "    # Use the earlier defined function to compute the list of scalar products of the verbs and compare with divide\n",
    "    prodlist1 = mf.averaged_product_list(simdict,'COHYPONYMS',file_name,\"std_dev\")\n",
    "\n",
    "    prodlist5 = mf.averaged_product_list(simdict,'HYPER/HYPONYMS',file_name,\"std_dev\")\n",
    "    \n",
    "    # Compare and count the number of synonyms we get right\n",
    "    syncomparison1 = [a > divide1 for a in list(prodlist1.values())]\n",
    "\n",
    "    # Compare and count the number of antonyms we get right\n",
    "    antcomparison1 = [a < divide1 for a in list(prodlist5.values())]\n",
    "\n",
    "    #Compute the average precision\n",
    "    precision1 = 1/2*(syncomparison1.count(True)/len(syncomparison1) + antcomparison1.count(True)/len(antcomparison1))\n",
    "    \n",
    "    return precision1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4ce9f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypohyper2(file_name,myseed1,myseed2,samplesize1,samplesize2):\n",
    "    #Since here we use just a subset of the pairs of synonyms and antonyms to determine the means and deviation, we have to do this by hand\n",
    "\n",
    "    #Compute the products for all the observable deviation vectors associated to the SYNONYMS pairs\n",
    "    all1 = mf.averaged_product_list(simdict,'COHYPONYMS',file_name,\"std_dev\")\n",
    "\n",
    "    #The subset from which we copute the means is randomly selected, we seed the random generator for repeatable results. Chosen seed 7\n",
    "    random.seed(myseed1)\n",
    "\n",
    "    mysample = random.sample(list(all1),samplesize1)\n",
    "\n",
    "    #Find complementary sample where we will predict and test\n",
    "    mycomplsample = set(all1.keys()) - set(mysample)\n",
    "\n",
    "    #Compute the means and standard deviations\n",
    "    syn1 = [np.mean(np.array(list(map(all1.get,mysample)))),np.std(np.array(list(map(all1.get,mysample))))]\n",
    "\n",
    "    #Now the same thing for the antonyms\n",
    "\n",
    "    #Compute the products for all the observable deviation vectors associated to the SYNONYMS pairs\n",
    "    all5 = mf.averaged_product_list(simdict,'HYPER/HYPONYMS',file_name,\"std_dev\")\n",
    "\n",
    "    #The subset from which we copute the means is randomly selected, we seed the random generator for repeatable results. Chosen seed 13\n",
    "    #random.seed(117) #Interesting values come out for this seed\n",
    "    random.seed(myseed2)\n",
    "\n",
    "    mysample2 = random.sample(list(all5),samplesize2)\n",
    "\n",
    "    #Find complementary sample where we will predict and test\n",
    "    mycomplsample2 = set(all5.keys()) - set(mysample2)\n",
    "\n",
    "    #Compute the means and standard deviations\n",
    "    ant1 = [np.mean(np.array(list(map(all5.get,mysample2)))),np.std(np.array(list(map(all5.get,mysample2))))]\n",
    "\n",
    "    #Use the means and deviations to set the divide between synonyms and antonyms\n",
    "    divide1 = ant1[0] + ant1[1]/syn1[1] * (syn1[0]-ant1[0])/2\n",
    "    \n",
    "    # Next we test on the complementary sample\n",
    "    # Compare and count the number of synonyms we get right\n",
    "    syncomparison1 = [a > divide1 for a in list(map(all1.get,mycomplsample))]\n",
    "\n",
    "    # Compare and count the number of antonyms we get right\n",
    "    antcomparison1 = [a < divide1 for a in list(map(all5.get,mycomplsample2))]\n",
    "\n",
    "    #Compute the average precision\n",
    "    precision1 = 1/2*(syncomparison1.count(True)/len(syncomparison1) + antcomparison1.count(True)/len(antcomparison1))\n",
    "\n",
    "    return precision1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c93ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73662424",
   "metadata": {},
   "source": [
    "## Choosing the seeds for the various samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ec06289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment 2 requires averaging over a number of different random samples. To generate these in a way \n",
    "#which is random but repeatable we need a list of seeds for the random generator. We decided to pick 20 samples\n",
    "\n",
    "#seed the random generator before genearting the integer seeds for the random samples\n",
    "random.seed(7)\n",
    "\n",
    "seed_list = [(random.randint(0,5000),random.randint(0,5000)) for a in range(20)]\n",
    "\n",
    "#Make a second seed_list for when we need 3 seeds instead of 2\n",
    "\n",
    "seed_list2 = [(random.randint(0,5000),random.randint(0,5000),random.randint(0,5000)) for a in range(20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2004f5",
   "metadata": {},
   "source": [
    "## 13 obs defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44fdb9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make observables object\n",
    "set1 = mf.observables([\"Linear.txt\",\"Quadratic.txt\"])\n",
    "# Load matrices and make vector dictionaries\n",
    "objsubset1 = mf.vector_dictionary(\"matrices_1160_arg_obj_context_subj.txt\",set1)\n",
    "subobjset1 = mf.vector_dictionary(\"matrices_1160_arg_subj_context_obj.txt\",set1,[0])\n",
    "obj08sub02set1 = mf.vector_dictionary([[\"matrices_1160_arg_obj_context_subj.txt\",\"matrices_1160_arg_subj_context_obj.txt\"],[0.8,0.2]],set1,[1])\n",
    "obj09sub01set1 = mf.vector_dictionary([[\"matrices_1160_arg_obj_context_subj.txt\",\"matrices_1160_arg_subj_context_obj.txt\"],[0.9,0.1]],set1,[1])\n",
    "\n",
    "file_list1 = [objsubset1,obj09sub01set1,obj08sub02set1,subobjset1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1621e1e7",
   "metadata": {},
   "source": [
    "## 15 obs defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd97263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make observables object\n",
    "set2 = mf.observables([\"Cubic1.txt\",\"Quartic1.txt\"])\n",
    "# Load matrices and make vector dictionaries\n",
    "objsubset2 = mf.vector_dictionary(\"matrices_1160_arg_obj_context_subj.txt\",set2)\n",
    "subobjset2 = mf.vector_dictionary(\"matrices_1160_arg_subj_context_obj.txt\",set2,[0])\n",
    "obj08sub02set2 = mf.vector_dictionary([[\"matrices_1160_arg_obj_context_subj.txt\",\"matrices_1160_arg_subj_context_obj.txt\"],[0.8,0.2]],set2,[1])\n",
    "obj09sub01set2 = mf.vector_dictionary([[\"matrices_1160_arg_obj_context_subj.txt\",\"matrices_1160_arg_subj_context_obj.txt\"],[0.9,0.1]],set2,[1])\n",
    "\n",
    "file_list2 = [objsubset2,obj09sub01set2,obj08sub02set2,subobjset2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e087b05a",
   "metadata": {},
   "source": [
    "## 28 obs defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "452ce360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make observables object\n",
    "set3 = mf.observables([\"Linear.txt\",\"Quadratic.txt\",\"Cubic1.txt\",\"Quartic1.txt\",\"Additional1.txt\"])\n",
    "# Load matrices and make vector dictionaries\n",
    "objsubset3 = mf.vector_dictionary(\"matrices_1160_arg_obj_context_subj.txt\",set3)\n",
    "subobjset3 = mf.vector_dictionary(\"matrices_1160_arg_subj_context_obj.txt\",set3,[0])\n",
    "obj08sub02set3 = mf.vector_dictionary([[\"matrices_1160_arg_obj_context_subj.txt\",\"matrices_1160_arg_subj_context_obj.txt\"],[0.8,0.2]],set3,[1])\n",
    "obj09sub01set3 = mf.vector_dictionary([[\"matrices_1160_arg_obj_context_subj.txt\",\"matrices_1160_arg_subj_context_obj.txt\"],[0.9,0.1]],set3,[1])\n",
    "\n",
    "file_list3 = [objsubset3,obj09sub01set3,obj08sub02set3,subobjset3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f65639",
   "metadata": {},
   "source": [
    "## Experiment 1 syn vs ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "81823307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5602808691043986,\n",
       " 0.5545398339515986,\n",
       " 0.5565712771595125,\n",
       " 0.5234499205087441]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1 synonym vs antonym\n",
    "[synvsant1(a) for a in file_list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96c4ee80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5787405052110934,\n",
       " 0.5684949655537891,\n",
       " 0.5754725313548843,\n",
       " 0.5537007595831125]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1 synonym vs antonym\n",
    "[synvsant1(a) for a in file_list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e06ac100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5562179826885709,\n",
       " 0.5607224871930754,\n",
       " 0.5779014308426074,\n",
       " 0.5635930047694753]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1 synonym vs antonym\n",
    "[synvsant1(a) for a in file_list3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16709587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5602808691043986,\n",
       "  0.5545398339515986,\n",
       "  0.5565712771595125,\n",
       "  0.5234499205087441],\n",
       " [0.5787405052110934,\n",
       "  0.5684949655537891,\n",
       "  0.5754725313548843,\n",
       "  0.5537007595831125],\n",
       " [0.5562179826885709,\n",
       "  0.5607224871930754,\n",
       "  0.5779014308426074,\n",
       "  0.5635930047694753]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1 synonym vs antonym\n",
    "[[synvsant1(a) for a in localf] for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1282234c",
   "metadata": {},
   "source": [
    "## Experiment 1 hyper/hypo vs cohypo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb272e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5248026315789474,\n",
       " 0.5194078947368421,\n",
       " 0.5286842105263159,\n",
       " 0.5505921052631579]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1 synonym vs antonym\n",
    "[hypohyper1(a) for a in file_list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6b90179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5328947368421053,\n",
       " 0.5220065789473685,\n",
       " 0.5242763157894736,\n",
       " 0.5445065789473684]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1 synonym vs antonym\n",
    "[hypohyper1(a) for a in file_list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c7656d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5319078947368421,\n",
       " 0.5445394736842105,\n",
       " 0.5438157894736843,\n",
       " 0.5655921052631578]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1 synonym vs antonym\n",
    "[hypohyper1(a) for a in file_list3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2034c938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5248026315789474,\n",
       "  0.5194078947368421,\n",
       "  0.5286842105263159,\n",
       "  0.5505921052631579],\n",
       " [0.5328947368421053,\n",
       "  0.5220065789473685,\n",
       "  0.5242763157894736,\n",
       "  0.5445065789473684],\n",
       " [0.5319078947368421,\n",
       "  0.5445394736842105,\n",
       "  0.5438157894736843,\n",
       "  0.5655921052631578]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1 synonym vs antonym\n",
    "[[hypohyper1(a) for a in localf] for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190ea59d",
   "metadata": {},
   "source": [
    "## Experiment 1 syn vs ant vs none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcd4a4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3597136103529964,\n",
       " 0.37129490582176006,\n",
       " 0.3766332730527103,\n",
       " 0.34676898871272266]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1 synonym vs antonym\n",
    "[synvsnonevsant1(a) for a in file_list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bae2670b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3824331906173338,\n",
       " 0.3728928473173997,\n",
       " 0.3738713700350529,\n",
       " 0.37076496411534776]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1 synonym vs antonym\n",
    "[synvsnonevsant1(a) for a in file_list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4f629af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3661160387503098,\n",
       " 0.379667109846138,\n",
       " 0.38460790123194216,\n",
       " 0.37446110847645364]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1 synonym vs antonym\n",
    "[synvsnonevsant1(a) for a in file_list3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb77065a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.3597136103529964,\n",
       "  0.37129490582176006,\n",
       "  0.3766332730527103,\n",
       "  0.34676898871272266],\n",
       " [0.3824331906173338,\n",
       "  0.3728928473173997,\n",
       "  0.3738713700350529,\n",
       "  0.37076496411534776],\n",
       " [0.3661160387503098,\n",
       "  0.379667109846138,\n",
       "  0.38460790123194216,\n",
       "  0.37446110847645364]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment 1 synonym vs antonym\n",
    "[[synvsnonevsant1(a) for a in localf] for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ef09c",
   "metadata": {},
   "source": [
    "## Experiment 2 syn vs ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c3901285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.55718477, 0.0286868 ],\n",
       "        [0.56323631, 0.03947724],\n",
       "        [0.56193626, 0.03038054],\n",
       "        [0.53352508, 0.02666175]]),\n",
       " array([[0.58256443, 0.02190353],\n",
       "        [0.5679878 , 0.01723463],\n",
       "        [0.56153935, 0.04105102],\n",
       "        [0.56240796, 0.03135284]]),\n",
       " array([[0.55715601, 0.03037697],\n",
       "        [0.57337782, 0.03124492],\n",
       "        [0.58143695, 0.02554353],\n",
       "        [0.57532789, 0.03366879]])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to every file with the given seed and average. The averaging is done using pure functions\n",
    "# (lambda) so to compute all the precisions only once\n",
    "\n",
    "# Synonyms vs Antonyms, sample size roughly 60%\n",
    "\n",
    "[(lambda x1 : np.array([np.mean(x1,axis = 0),np.std(x1,axis=0)]).T)([[synvsant2(myfile,*a,200,70) for myfile in localf] for a in seed_list ]) for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fff8883d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.55313735, 0.01489417],\n",
       "        [0.56247255, 0.0131478 ],\n",
       "        [0.56048803, 0.01326686],\n",
       "        [0.53536726, 0.01787474]]),\n",
       " array([[0.58037989, 0.01292189],\n",
       "        [0.57164855, 0.01582666],\n",
       "        [0.56485782, 0.01662805],\n",
       "        [0.5471289 , 0.00887981]]),\n",
       " array([[0.55912659, 0.01588535],\n",
       "        [0.57385815, 0.01515517],\n",
       "        [0.57766524, 0.01564467],\n",
       "        [0.55278601, 0.02270612]])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synonyms vs Antonyms, sample size roughly 10%\n",
    "\n",
    "[(lambda x1 : np.array([np.mean(x1,axis = 0),np.std(x1,axis=0)]).T)([[synvsant2(myfile,*a,30,12) for myfile in localf] for a in seed_list ]) for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c195ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.5546809 , 0.01317812],\n",
       "        [0.56127639, 0.01222263],\n",
       "        [0.56027491, 0.0118855 ],\n",
       "        [0.54448208, 0.02198171]]),\n",
       " array([[0.5769784 , 0.01177556],\n",
       "        [0.57148012, 0.0158921 ],\n",
       "        [0.56489445, 0.02028405],\n",
       "        [0.55553019, 0.01720257]]),\n",
       " array([[0.56086892, 0.01719457],\n",
       "        [0.57201522, 0.01965759],\n",
       "        [0.57795533, 0.01903933],\n",
       "        [0.56073638, 0.02140626]])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synonyms vs Antonyms, sample size roughly 5%\n",
    "\n",
    "[(lambda x1 : np.array([np.mean(x1,axis = 0),np.std(x1,axis=0)]).T)([[synvsant2(myfile,*a,15,6) for myfile in localf] for a in seed_list ]) for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4e112a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.5577266 , 0.01389964],\n",
       "        [0.56288629, 0.00989336],\n",
       "        [0.56156131, 0.0092551 ],\n",
       "        [0.55015436, 0.02242509]]),\n",
       " array([[0.57596063, 0.02098345],\n",
       "        [0.57226697, 0.02336295],\n",
       "        [0.56572667, 0.02486098],\n",
       "        [0.55169404, 0.02936456]]),\n",
       " array([[0.56332978, 0.01718888],\n",
       "        [0.57100389, 0.01619591],\n",
       "        [0.57198489, 0.01973936],\n",
       "        [0.55780731, 0.02383717]])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synonyms vs Antonyms, sample size 5 pairs\n",
    "\n",
    "[(lambda x1 : np.array([np.mean(x1,axis = 0),np.std(x1,axis=0)]).T)([[synvsant2(myfile,*a,5,5) for myfile in localf] for a in seed_list ]) for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c7811",
   "metadata": {},
   "source": [
    "## Experiment 2 hyper/hypo vs cohypo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc8e00ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.50963816, 0.02529933],\n",
       "        [0.5069778 , 0.03434824],\n",
       "        [0.51860197, 0.02783025],\n",
       "        [0.54222451, 0.0201456 ]]),\n",
       " array([[0.52762336, 0.01957085],\n",
       "        [0.51708882, 0.01936838],\n",
       "        [0.52254934, 0.02614483],\n",
       "        [0.53748355, 0.02324084]]),\n",
       " array([[0.52259457, 0.02712426],\n",
       "        [0.52703947, 0.0302006 ],\n",
       "        [0.53069901, 0.02741803],\n",
       "        [0.56026316, 0.02164468]])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to every file with the given seed and average. The averaging is done using pure functions\n",
    "# (lambda) so to compute all the precisions only once\n",
    "\n",
    "# Synonyms vs Antonyms, sample size roughly 60%\n",
    "\n",
    "[(lambda x1 : np.array([np.mean(x1,axis = 0),np.std(x1,axis=0)]).T)([[hypohyper2(myfile,*a,114,480) for myfile in localf] for a in seed_list ]) for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53094443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.52357273, 0.01338966],\n",
       "        [0.52379751, 0.01891544],\n",
       "        [0.52577668, 0.01569851],\n",
       "        [0.55484284, 0.01587564]]),\n",
       " array([[0.52902778, 0.00848917],\n",
       "        [0.52448099, 0.00567904],\n",
       "        [0.53323282, 0.00684499],\n",
       "        [0.54699379, 0.00603373]]),\n",
       " array([[0.53230629, 0.00869123],\n",
       "        [0.53177083, 0.01318194],\n",
       "        [0.53851425, 0.0116434 ],\n",
       "        [0.56395833, 0.01211284]])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synonyms vs Antonyms, sample size roughly 10%\n",
    "\n",
    "[(lambda x1 : np.array([np.mean(x1,axis = 0),np.std(x1,axis=0)]).T)([[hypohyper2(myfile,*a,19,80) for myfile in localf] for a in seed_list ]) for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8b242de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.52189608, 0.01030549],\n",
       "        [0.52167309, 0.01442212],\n",
       "        [0.52569788, 0.01094266],\n",
       "        [0.55201821, 0.00989789]]),\n",
       " array([[0.52883051, 0.01047954],\n",
       "        [0.52650207, 0.00691944],\n",
       "        [0.53402806, 0.00578395],\n",
       "        [0.54659167, 0.00435978]]),\n",
       " array([[0.53271245, 0.00648235],\n",
       "        [0.53048997, 0.01184247],\n",
       "        [0.5345342 , 0.00833232],\n",
       "        [0.56251872, 0.01063493]])]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synonyms vs Antonyms, sample size roughly 5%\n",
    "\n",
    "[(lambda x1 : np.array([np.mean(x1,axis = 0),np.std(x1,axis=0)]).T)([[hypohyper2(myfile,*a,9,40) for myfile in localf] for a in seed_list ]) for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "07ab4d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.5168409 , 0.01241783],\n",
       "        [0.51499235, 0.01568301],\n",
       "        [0.51913989, 0.01263556],\n",
       "        [0.54879143, 0.01070788]]),\n",
       " array([[0.52899286, 0.01248835],\n",
       "        [0.52774095, 0.0097494 ],\n",
       "        [0.53356026, 0.00652065],\n",
       "        [0.54519208, 0.01666236]]),\n",
       " array([[0.52602329, 0.00781675],\n",
       "        [0.52432857, 0.01223066],\n",
       "        [0.53190719, 0.01100075],\n",
       "        [0.55731939, 0.00973795]])]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synonyms vs Antonyms, sample size 5 pairs\n",
    "\n",
    "[(lambda x1 : np.array([np.mean(x1,axis = 0),np.std(x1,axis=0)]).T)([[hypohyper2(myfile,*a,5,5) for myfile in localf] for a in seed_list ]) for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c2572",
   "metadata": {},
   "source": [
    "## Experiment 2 syn s none vs ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc8574d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.36039578, 0.02427392],\n",
       "        [0.3705463 , 0.02603444],\n",
       "        [0.37172346, 0.02522594],\n",
       "        [0.35021321, 0.02605001]]),\n",
       " array([[0.3847989 , 0.02716517],\n",
       "        [0.38569239, 0.02671016],\n",
       "        [0.38778735, 0.02861508],\n",
       "        [0.37448965, 0.0212687 ]]),\n",
       " array([[0.3692036 , 0.02766112],\n",
       "        [0.37458247, 0.02750455],\n",
       "        [0.3810474 , 0.02523456],\n",
       "        [0.37123831, 0.0240733 ]])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synonyms vs Antonyms, sample size 60%\n",
    "\n",
    "[(lambda x1 : np.array([np.mean(x1,axis = 0),np.std(x1,axis=0)]).T)([[synvsnonevsant2(myfile,*a,200,70,1360) for myfile in localf] for a in seed_list2 ]) for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "253e3d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.36658493, 0.01510189],\n",
       "        [0.37007301, 0.01209757],\n",
       "        [0.3705702 , 0.01027956],\n",
       "        [0.35227173, 0.00974955]]),\n",
       " array([[0.38692058, 0.01271578],\n",
       "        [0.38058816, 0.01765156],\n",
       "        [0.38414715, 0.01606983],\n",
       "        [0.36916266, 0.00980572]]),\n",
       " array([[0.36776551, 0.01318403],\n",
       "        [0.37679871, 0.01156412],\n",
       "        [0.38452368, 0.01081359],\n",
       "        [0.36400991, 0.0150756 ]])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synonyms vs Antonyms, sample size 10%\n",
    "\n",
    "[(lambda x1 : np.array([np.mean(x1,axis = 0),np.std(x1,axis=0)]).T)([[synvsnonevsant2(myfile,*a,30,12,225) for myfile in localf] for a in seed_list2 ]) for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1a0c024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.38210355, 0.02758579],\n",
       "        [0.38353394, 0.02454306],\n",
       "        [0.38295199, 0.01871915],\n",
       "        [0.36219333, 0.02644928]]),\n",
       " array([[0.38847314, 0.01290714],\n",
       "        [0.38332315, 0.02182457],\n",
       "        [0.3821228 , 0.01898871],\n",
       "        [0.3709394 , 0.00898255]]),\n",
       " array([[0.38031931, 0.02803658],\n",
       "        [0.38773164, 0.02059527],\n",
       "        [0.39052781, 0.0174915 ],\n",
       "        [0.36314847, 0.02351678]])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synonyms vs Antonyms, sample size 5%\n",
    "\n",
    "[(lambda x1 : np.array([np.mean(x1,axis = 0),np.std(x1,axis=0)]).T)([[synvsnonevsant2(myfile,*a,15,6,112) for myfile in localf] for a in seed_list2 ]) for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1556b352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.37902192, 0.03132735],\n",
       "        [0.38369504, 0.03117066],\n",
       "        [0.38337985, 0.03059548],\n",
       "        [0.36826069, 0.03048114]]),\n",
       " array([[0.39106474, 0.02427315],\n",
       "        [0.3903828 , 0.03170344],\n",
       "        [0.38823387, 0.03175568],\n",
       "        [0.39318451, 0.03814151]]),\n",
       " array([[0.38331792, 0.03750307],\n",
       "        [0.39103096, 0.0460884 ],\n",
       "        [0.39138439, 0.04303463],\n",
       "        [0.36960936, 0.03221026]])]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synonyms vs Antonyms, sample size 5 pairs\n",
    "\n",
    "[(lambda x1 : np.array([np.mean(x1,axis = 0),np.std(x1,axis=0)]).T)([[synvsnonevsant2(myfile,*a,5,5,5) for myfile in localf] for a in seed_list2 ]) for localf in [file_list1,file_list2,file_list3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d67b7de",
   "metadata": {},
   "source": [
    "## Precision through sorting over same sample sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecedd927",
   "metadata": {},
   "source": [
    "### Syn vs Ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "41806ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to take all synonyms and antonyms, sort them, set the divide to the number of antonyms n and check\n",
    "# how many of the first n are actually antonyms\n",
    "def precision_by_sorting_synant(file_name):\n",
    "    \n",
    "    ants = mf.averaged_product_list(simdict,'ANTONYMS',file_name,\"std_dev\")\n",
    "    syns = {}\n",
    "    test2 = mf.averaged_product_list(simdict,'SYNONYMS',file_name,\"std_dev\")\n",
    "    for a in random.sample(list(test2.keys()),len(ants)):\n",
    "        syns.update({a : test2[a]})\n",
    "    \n",
    "    #allverbs = syns.update(ants)\n",
    "    allverbs = syns | ants\n",
    "    \n",
    "    # Take the list of values from this dictionary and sort according to length\n",
    "    alllengths = allverbs.values()\n",
    "    # Sort by length\n",
    "    alllengths = sorted(alllengths)\n",
    "    \n",
    "    #Check how many of the first n (length of the antonyms) are actually antonyms and how many of the following\n",
    "    # are none\n",
    "    allantvals = ants.values()\n",
    "    allsynvals = syns.values()\n",
    "    #print([set(allantvals).intersection(set(allnonvals)),set(allantvals).intersection(set(allsynvals)),set(allsynvals).intersection(set(allnonvals))])\n",
    "    mycounts = []\n",
    "    \n",
    "    for a in alllengths[:len(allantvals)]:\n",
    "        mycounts.append(a in allantvals)\n",
    "    \n",
    "    local = mycounts.count(True)\n",
    "    #print(local/len(allantvals))\n",
    "        \n",
    "    for a in alllengths[-len(allsynvals):]:\n",
    "        mycounts.append(a in allsynvals)\n",
    "    \n",
    "    local = mycounts.count(True)-local\n",
    "    #print(local/len(allsynvals))\n",
    "    \n",
    "    # Count the trues\n",
    "    mycounts = mycounts.count(True)\n",
    "    \n",
    "    # Compute precision, and recycle variables which I do not need anymore\n",
    "    \n",
    "    allantvals = mycounts/(len(allsynvals) + len(allantvals))\n",
    "    \n",
    "    return allantvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "523c1fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_by_sorting_synant_average(file_name,iterations,myseed):\n",
    "    \n",
    "    random.seed(myseed)\n",
    "    \n",
    "    mylist = []\n",
    "    for a in range(iterations):\n",
    "        mylist.append(precision_by_sorting_synant(file_name))\n",
    "    \n",
    "    return [np.mean(np.array(mylist)),np.std(np.array(mylist))]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ddc7dfdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5554054054054053, 0.01762522301487733]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_by_sorting_synant_average(objsubset1,20,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e0167f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5554054054054053, 0.01762522301487733],\n",
       " [0.572072072072072, 0.016241221961549496],\n",
       " [0.5657657657657656, 0.014693249036306387],\n",
       " [0.549099099099099, 0.019421676700279365]]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[precision_by_sorting_synant_average(a,20,33) for a in file_list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9027400b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5734234234234235, 0.023540076811628684],\n",
       " [0.572072072072072, 0.016974273586861967],\n",
       " [0.5819819819819819, 0.01718809371922426],\n",
       " [0.568018018018018, 0.017899384221896866]]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[precision_by_sorting_synant_average(a,20,33) for a in file_list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8adea7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5554054054054054, 0.01618490470560511],\n",
       " [0.5680180180180182, 0.020833028972697624],\n",
       " [0.5774774774774775, 0.02130016291448088],\n",
       " [0.5698198198198198, 0.021295399375806875]]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[precision_by_sorting_synant_average(a,20,33) for a in file_list3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b28abe",
   "metadata": {},
   "source": [
    "### Hyper/hypo vs cohypo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "990f792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to take all synonyms and antonyms, sort them, set the divide to the number of antonyms n and check\n",
    "# how many of the first n are actually antonyms\n",
    "def precision_by_sorting_hyper(file_name):\n",
    "    \n",
    "    ants = mf.averaged_product_list(simdict,'COHYPONYMS',file_name,\"std_dev\")\n",
    "    syns = {}\n",
    "    test2 = mf.averaged_product_list(simdict,'HYPER/HYPONYMS',file_name,\"std_dev\")\n",
    "    for a in random.sample(list(test2.keys()),len(ants)):\n",
    "        syns.update({a : test2[a]})\n",
    "        \n",
    "    (ants,syns) = (syns,ants)\n",
    "    \n",
    "    #allverbs = syns.update(ants)\n",
    "    allverbs = syns | ants\n",
    "    \n",
    "    # Take the list of values from this dictionary and sort according to length\n",
    "    alllengths = allverbs.values()\n",
    "    # Sort by length\n",
    "    alllengths = sorted(alllengths)\n",
    "    \n",
    "    #Check how many of the first n (length of the antonyms) are actually antonyms and how many of the following\n",
    "    # are none\n",
    "    allantvals = ants.values()\n",
    "    allsynvals = syns.values()\n",
    "    #print([set(allantvals).intersection(set(allnonvals)),set(allantvals).intersection(set(allsynvals)),set(allsynvals).intersection(set(allnonvals))])\n",
    "    mycounts = []\n",
    "    \n",
    "    for a in alllengths[:len(allantvals)]:\n",
    "        mycounts.append(a in allantvals)\n",
    "    \n",
    "    local = mycounts.count(True)\n",
    "    #print(local/len(allantvals))\n",
    "        \n",
    "    for a in alllengths[-len(allsynvals):]:\n",
    "        mycounts.append(a in allsynvals)\n",
    "    \n",
    "    local = mycounts.count(True)-local\n",
    "    #print(local/len(allsynvals))\n",
    "    \n",
    "    # Count the trues\n",
    "    mycounts = mycounts.count(True)\n",
    "    \n",
    "    # Compute precision, and recycle variables which I do not need anymore\n",
    "    \n",
    "    allantvals = mycounts/(len(allsynvals) + len(allantvals))\n",
    "    \n",
    "    return allantvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "98311b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_by_sorting_hyper_average(file_name,iterations,myseed):\n",
    "    \n",
    "    random.seed(myseed)\n",
    "    \n",
    "    mylist = []\n",
    "    for a in range(iterations):\n",
    "        mylist.append(precision_by_sorting_hyper(file_name))\n",
    "    \n",
    "    return [np.mean(np.array(mylist)),np.std(np.array(mylist))]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "726002bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5239473684210527, 0.01600510652582628]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_by_sorting_hyper_average(objsubset1,20,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8b8fbde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5239473684210527, 0.01600510652582628],\n",
       " [0.5192105263157895, 0.019500337378997756],\n",
       " [0.5276315789473685, 0.018967460107676214],\n",
       " [0.551578947368421, 0.019664780728704628]]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[precision_by_sorting_hyper_average(a,20,33) for a in file_list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bcef5cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5228947368421053, 0.014051251657065784],\n",
       " [0.5315789473684212, 0.015163010832513635],\n",
       " [0.5326315789473685, 0.017184029176120535],\n",
       " [0.5594736842105263, 0.022211524048430725]]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[precision_by_sorting_hyper_average(a,20,33) for a in file_list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1e6e9cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5331578947368422, 0.014708619592086455],\n",
       " [0.5371052631578948, 0.016848271897378028],\n",
       " [0.5436842105263158, 0.020927258265337947],\n",
       " [0.5652631578947369, 0.020000000000000004]]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[precision_by_sorting_hyper_average(a,20,33) for a in file_list3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77679b65",
   "metadata": {},
   "source": [
    "### Syn vs Ant vs None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "e0e0c711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to take all synonyms and antonyms, sort them, set the divide to the number of antonyms n and check\n",
    "# how many of the first n are actually antonyms\n",
    "def precision_by_sorting_synnonant(file_name):\n",
    "    \n",
    "    ants = mf.averaged_product_list(simdict,'ANTONYMS',file_name,\"std_dev\")\n",
    "    syns = {}\n",
    "    test2 = mf.averaged_product_list(simdict,'SYNONYMS',file_name,\"std_dev\")\n",
    "    for a in random.sample(list(test2.keys()),len(ants)):\n",
    "        syns.update({a : test2[a]})\n",
    "    nons = {}\n",
    "    test2 = mf.averaged_product_list(simdict,'NONE',file_name,\"std_dev\")\n",
    "    for a in random.sample(list(test2.keys()),len(ants)):\n",
    "        nons.update({a : test2[a]})\n",
    "    \n",
    "    #allverbs = syns.update(ants)\n",
    "    allverbs = syns | nons | ants\n",
    "    \n",
    "    # Take the list of values from this dictionary and sort according to length\n",
    "    alllengths = allverbs.values()\n",
    "    # Sort by length\n",
    "    alllengths = sorted(alllengths)\n",
    "    \n",
    "    #Check how many of the first n (length of the antonyms) are actually antonyms and how many of the following\n",
    "    # are none\n",
    "    allantvals = ants.values()\n",
    "    allnonvals = list(nons.values())[:]\n",
    "    allsynvals = syns.values()\n",
    "    #print([set(allantvals).intersection(set(allnonvals)),set(allantvals).intersection(set(allsynvals)),set(allsynvals).intersection(set(allnonvals))])\n",
    "    mycounts = []\n",
    "    \n",
    "    for a in alllengths[:len(ants)]:\n",
    "        mycounts.append(a in allantvals)\n",
    "    local = mycounts.count(True)\n",
    "    #print(local/len(ants))\n",
    "        \n",
    "    for a in alllengths[len(ants) + 1:len(ants) + len(nons)]:\n",
    "        mycounts.append(a in allnonvals)\n",
    "    local = mycounts.count(True)-local\n",
    "    #print(local/len(nons))\n",
    "        \n",
    "    for a in alllengths[-len(allsynvals):]:\n",
    "        mycounts.append(a in allsynvals)\n",
    "    local = mycounts.count(True)-local\n",
    "    #print(local/len(syns))\n",
    "    \n",
    "    # Count the trues\n",
    "    mycounts = mycounts.count(True)\n",
    "    \n",
    "    # Compute precision, and recycle variables which I do not need anymore\n",
    "    \n",
    "    allantvals = mycounts/(len(allsynvals) + len(allantvals) + len(allnonvals))\n",
    "    \n",
    "    return allantvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d72528d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_by_sorting_synnonant_average(file_name,iterations,myseed):\n",
    "    \n",
    "    random.seed(myseed)\n",
    "    \n",
    "    mylist = []\n",
    "    for a in range(iterations):\n",
    "        mylist.append(precision_by_sorting_synnonant(file_name))\n",
    "    \n",
    "    return [np.mean(np.array(mylist)),np.std(np.array(mylist))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "d38f74c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.27927927927927926\n",
      "0.7297297297297297\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33633633633633636"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_by_sorting_synnonant(objsubset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "88e20dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3546546546546547, 0.019108634558478117]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_by_sorting_synnonant_average(objsubset1,20,33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "14a359fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.3546546546546547, 0.019108634558478117],\n",
       " [0.36396396396396397, 0.0217712540064767],\n",
       " [0.36036036036036034, 0.02175882394322738],\n",
       " [0.3355855855855856, 0.020715823941562014]]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[precision_by_sorting_synnonant_average(a,20,33) for a in file_list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ee5560df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.3536036036036036, 0.021210535678306143],\n",
       " [0.35900900900900906, 0.018498961618164403],\n",
       " [0.35435435435435436, 0.018992658619629902],\n",
       " [0.3256756756756756, 0.0216648508455183]]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[precision_by_sorting_synnonant_average(a,20,33) for a in file_list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "52233d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.3454954954954955, 0.01667883655997623],\n",
       " [0.34894894894894896, 0.014730028190307174],\n",
       " [0.35015015015015016, 0.01697161699813188],\n",
       " [0.32387387387387384, 0.022677643986932578]]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[precision_by_sorting_synnonant_average(a,20,33) for a in file_list3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba7bbd1",
   "metadata": {},
   "source": [
    "## Precision on human judgment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "27a66335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.96"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First extract the largest value and use it to normalise the rest\n",
    "allvals=np.array([])\n",
    "for a in simdict.keys():\n",
    "    allvals = np.concatenate((allvals,np.array(simdict[a]).T[2]))\n",
    "allvals = np.array([float(a) for a in allvals])\n",
    "highest = max(allvals)\n",
    "highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "391ac521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all values for the synonym pairs and normalise\n",
    "allsyn =np.array([float(a) for a in np.array(simdict['SYNONYMS']).T[2]])\n",
    "allsyn = allsyn / highest\n",
    "#Same for none\n",
    "allnon =np.array([float(a) for a in np.array(simdict['NONE']).T[2]])\n",
    "allnon = allnon / highest\n",
    "#Same for anotnysm\n",
    "allant =np.array([float(a) for a in np.array(simdict['ANTONYMS']).T[2]])\n",
    "allant = allant / highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1a1e7d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6816415990760427, 0.210948684438062] [0.3445055904458733, 0.23515413813688968] [0.09816744455298675, 0.1073676682566156]\n"
     ]
    }
   ],
   "source": [
    "# Compute mean and deviations\n",
    "syn1 = [np.mean(allsyn),np.std(allsyn)]\n",
    "non1 = [np.mean(allnon),np.std(allnon)]\n",
    "ant1 = [np.mean(allant),np.std(allant)]\n",
    "print(syn1,non1,ant1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d6fe33c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24665440863627003\n",
      "0.947242206235012\n"
     ]
    }
   ],
   "source": [
    "#Use the means and deviations to set the divide between synonyms and antonyms\n",
    "divide1 = ant1[0] + ant1[1]/syn1[1] * (syn1[0]-ant1[0])/2\n",
    "\n",
    "# Compare and count the number of synonyms we get right\n",
    "syncomparison1 = [abs(a) > divide1 for a in allsyn]\n",
    "\n",
    "# Compare and count the number of antonyms we get right\n",
    "antcomparison1 = [abs(a) < divide1 for a in allant]\n",
    "\n",
    "precision1 = (syncomparison1.count(True) + antcomparison1.count(True))/(len(syncomparison1) + len(antcomparison1))\n",
    "\n",
    "print(divide1)\n",
    "print(precision1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a9eaadaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5324160478055069\n",
      "0.15440449939864861\n",
      "0.49721115537848604\n"
     ]
    }
   ],
   "source": [
    "#Use the means and deviations to set the divide between synonyms and none\n",
    "divide1 = non1[0] + non1[1]/syn1[1] * (syn1[0]-non1[0])/2\n",
    "\n",
    "#Use the means and deviations to set the divide between none and antonyms\n",
    "divide2 = ant1[0] + ant1[1]/non1[1] * (non1[0]-ant1[0])/2\n",
    "\n",
    "# Compare and count the number of synonyms we get right\n",
    "syncomparison1 = [abs(a) > divide1 for a in allsyn]\n",
    "\n",
    "# Compare and count the number of none we get right\n",
    "noncomparison1 = [abs(a) > divide2 and abs(a) < divide1 for a in allnon]\n",
    "\n",
    "# Compare and count the number of antonyms we get right\n",
    "antcomparison1 = [abs(a) < divide2 for a in allant]\n",
    "\n",
    "precision1 = (syncomparison1.count(True) + antcomparison1.count(True) + noncomparison1.count(True))/(len(syncomparison1) + len(antcomparison1) + len(noncomparison1))\n",
    "\n",
    "print(divide1)\n",
    "print(divide2)\n",
    "print(precision1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "547ef2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define similar functions as previously but we needed some tweeked input\n",
    "\n",
    "allsyndict = {}\n",
    "for a in simdict['SYNONYMS']:\n",
    "    allsyndict.update({(a[0],a[1]):a[2]/highest})\n",
    "    \n",
    "allnondict = {}\n",
    "for a in simdict['NONE']:\n",
    "    allnondict.update({(a[0],a[1]):a[2]/highest})\n",
    "    \n",
    "allantdict = {}\n",
    "for a in simdict['ANTONYMS']:\n",
    "    allantdict.update({(a[0],a[1]):a[2]/highest})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "25ef1b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human1(myseed1,myseed2,samplesize1,samplesize2):\n",
    "    \n",
    "    #Compute the products for all the observable deviation vectors associated to the SYNONYMS pairs\n",
    "    all1 = allsyndict\n",
    "\n",
    "    #The subset from which we copute the means is randomly selected, we seed the random generator for repeatable results. Chosen seed 7\n",
    "    random.seed(myseed1)\n",
    "\n",
    "    mysample = random.sample(list(all1),samplesize1)\n",
    "\n",
    "    #Find complementary sample where we will predict and test\n",
    "    mycomplsample = set(all1.keys()) - set(mysample)\n",
    "\n",
    "    #Compute the means and standard deviations\n",
    "    syn1 = [np.mean(np.array(list(map(all1.get,mysample)))),np.std(np.array(list(map(all1.get,mysample))))]\n",
    "\n",
    "    #Now the same thing for the antonyms\n",
    "\n",
    "    #Compute the products for all the observable deviation vectors associated to the SYNONYMS pairs\n",
    "    all5 = allantdict\n",
    "\n",
    "    #The subset from which we copute the means is randomly selected, we seed the random generator for repeatable results. Chosen seed 13\n",
    "    #random.seed(117) #Interesting values come out for this seed\n",
    "    random.seed(myseed2)\n",
    "\n",
    "    mysample2 = random.sample(list(all5),samplesize2)\n",
    "\n",
    "    #Find complementary sample where we will predict and test\n",
    "    mycomplsample2 = set(all5.keys()) - set(mysample2)\n",
    "\n",
    "    #Compute the means and standard deviations\n",
    "    ant1 = [np.mean(np.array(list(map(all5.get,mysample2)))),np.std(np.array(list(map(all5.get,mysample2))))]\n",
    "\n",
    "    #Use the means and deviations to set the divide between synonyms and antonyms\n",
    "    divide1 = ant1[0] + ant1[1]/syn1[1] * (syn1[0]-ant1[0])/2\n",
    "    \n",
    "    # Next we test on the complementary sample\n",
    "    # Compare and count the number of synonyms we get right\n",
    "    syncomparison1 = [a > divide1 for a in list(map(all1.get,mycomplsample))]\n",
    "\n",
    "    # Compare and count the number of antonyms we get right\n",
    "    antcomparison1 = [a < divide1 for a in list(map(all5.get,mycomplsample2))]\n",
    "\n",
    "    #Compute the average precision\n",
    "    precision1 = (syncomparison1.count(True) + antcomparison1.count(True))/(len(syncomparison1) + len(antcomparison1))\n",
    "\n",
    "    return precision1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bee6f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human2(myseed1,myseed2,myseed3,samplesize1,samplesize2,samplesize3):\n",
    "    \n",
    "    #Since here we use just a subset of the pairs of synonyms and antonyms to determine the means and deviation, we have to do this by hand\n",
    "\n",
    "    #Compute the products for all the observable deviation vectors associated to the SYNONYMS pairs\n",
    "    all1 = allsyndict\n",
    "\n",
    "    #The subset from which we copute the means is randomly selected, we seed the random generator for repeatable results. Chosen seed 7\n",
    "    random.seed(myseed1)\n",
    "\n",
    "    mysample = random.sample(list(all1),samplesize1)\n",
    "\n",
    "    #Find complementary sample where we will predict and test\n",
    "    mycomplsample = set(all1.keys()) - set(mysample)\n",
    "\n",
    "    #Compute the means and standard deviations\n",
    "    syn1 = [np.mean(np.array(list(map(all1.get,mysample)))),np.std(np.array(list(map(all1.get,mysample))))]\n",
    "\n",
    "    #Now the same thing for the antonyms\n",
    "\n",
    "    #Compute the products for all the observable deviation vectors associated to the SYNONYMS pairs\n",
    "    all5 = allantdict\n",
    "    \n",
    "    #The subset from which we copute the means is randomly selected, we seed the random generator for repeatable results. Chosen seed 13\n",
    "    random.seed(myseed2)\n",
    "\n",
    "    mysample2 = random.sample(list(all5),samplesize2)\n",
    "\n",
    "    #Find complementary sample where we will predict and test\n",
    "    mycomplsample2 = set(all5.keys()) - set(mysample2)\n",
    "\n",
    "    #Compute the means and standard deviations\n",
    "    ant1 = [np.mean(np.array(list(map(all5.get,mysample2)))),np.std(np.array(list(map(all5.get,mysample2))))]\n",
    "    \n",
    "    #Finally the same for None\n",
    "\n",
    "    #Compute the products for all the observable deviation vectors associated to the SYNONYMS pairs\n",
    "    all9 = allnondict\n",
    "    \n",
    "    #The subset from which we copute the means is randomly selected, we seed the random generator for repeatable results. Chosen seed 13\n",
    "    random.seed(myseed3)\n",
    "\n",
    "    mysample3 = random.sample(list(all9),samplesize3)\n",
    "\n",
    "    #Find complementary sample where we will predict and test\n",
    "    mycomplsample3 = set(all9.keys()) - set(mysample3)\n",
    "\n",
    "    #Compute the means and standard deviations\n",
    "    non1 = [np.mean(np.array(list(map(all9.get,mysample3)))),np.std(np.array(list(map(all9.get,mysample3))))]\n",
    "    \n",
    "    #Use the means and deviations to set the divide between synonyms and none\n",
    "    divide1 = non1[0] + non1[1]/syn1[1] * (syn1[0]-non1[0])/2\n",
    "\n",
    "    #Use the means and deviations to set the divide between none and antonyms\n",
    "    divide5 = ant1[0] + ant1[1]/non1[1] * (non1[0]-ant1[0])/2\n",
    "    \n",
    "    # Next we test on the complementary sample\n",
    "    # Compare and count the number of synonyms we get right\n",
    "    syncomparison1 = [a > divide1 for a in list(map(all1.get,mycomplsample))]\n",
    "\n",
    "\n",
    "    # Compare and count the number of none we get right\n",
    "    noncomparison1 = [a > divide5 and a < divide1 for a in list(map(all9.get,mycomplsample3))]\n",
    "    \n",
    "    # Compare and count the number of antonyms we get right\n",
    "    antcomparison1 = [a < divide5 for a in list(map(all5.get,mycomplsample2))]\n",
    "    \n",
    "    #Compute the average precision\n",
    "    precision1 = (syncomparison1.count(True) + antcomparison1.count(True) + noncomparison1.count(True))/(len(syncomparison1) + len(antcomparison1) + len(noncomparison1))\n",
    "    \n",
    "    return precision1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a847ea9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9452380952380952, 0.013515861555309856]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to every file with the given seed and average. The averaging is done using pure functions\n",
    "# (lambda) so to compute all the precisions only once\n",
    "\n",
    "# Synonyms vs Antonyms, sample size roughly 60%\n",
    "\n",
    "(lambda x1 : [np.mean(x1,axis = 0),np.std(x1,axis=0)])([human1(*a,200,70) for a in seed_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2a943588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5046590909090909, 0.017094248343998072]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Synonyms vs None vs Antonyms, sample size roughly 60%\n",
    "\n",
    "(lambda x1 : [np.mean(x1,axis = 0),np.std(x1,axis=0)])([human2(*a,200,70,1360) for a in seed_list2 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c7b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
